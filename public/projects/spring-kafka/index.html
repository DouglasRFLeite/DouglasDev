<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Tutorial - Spring Boot & Apache Kafka | DouglasDev</title>
<meta name=keywords content="Java,Spring,Spring Boot,Apache Kafka,Event Streaming,Producer,Consumer"><meta name=description content="A tutorial on how I created two Spring Boot applications that comunicate via Apache Kafka"><meta name=author content="Douglas"><link rel=canonical href=https://canonical.url/to/page><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.eb010bce980c6450595d968d3d88ebc4a8b3750ec5bb5cbdef57d4f2f642da6f.css integrity="sha256-6wELzpgMZFBZXZaNPYjrxKizdQ7Fu1y971fU8vZC2m8=" rel="preload stylesheet" as=style><link rel=icon href=https://douglas-rocha-dev.vercel.app/images/code-icon.png><link rel=icon type=image/png sizes=16x16 href=https://douglas-rocha-dev.vercel.app/%3Clink%20/%20abs%20url%3E><link rel=icon type=image/png sizes=32x32 href=https://douglas-rocha-dev.vercel.app/%3Clink%20/%20abs%20url%3E><link rel=apple-touch-icon href=https://douglas-rocha-dev.vercel.app/%3Clink%20/%20abs%20url%3E><link rel=mask-icon href=https://douglas-rocha-dev.vercel.app/%3Clink%20/%20abs%20url%3E><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://douglas-rocha-dev.vercel.app/projects/spring-kafka/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="https://douglas-rocha-dev.vercel.app/projects/spring-kafka/"><meta property="og:site_name" content="DouglasDev"><meta property="og:title" content="Tutorial - Spring Boot & Apache Kafka"><meta property="og:description" content="A tutorial on how I created two Spring Boot applications that comunicate via Apache Kafka"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="projects"><meta property="article:published_time" content="2025-04-17T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-17T00:00:00+00:00"><meta property="article:tag" content="Java"><meta property="article:tag" content="Spring"><meta property="article:tag" content="Spring Boot"><meta property="article:tag" content="Apache Kafka"><meta property="article:tag" content="Event Streaming"><meta property="article:tag" content="Producer"><meta property="og:image" content="https://douglas-rocha-dev.vercel.app/images/cool-setup-002.jpeg"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://douglas-rocha-dev.vercel.app/images/cool-setup-002.jpeg"><meta name=twitter:title content="Tutorial - Spring Boot & Apache Kafka"><meta name=twitter:description content="A tutorial on how I created two Spring Boot applications that comunicate via Apache Kafka"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://douglas-rocha-dev.vercel.app/projects/"},{"@type":"ListItem","position":2,"name":"Tutorial - Spring Boot \u0026 Apache Kafka","item":"https://douglas-rocha-dev.vercel.app/projects/spring-kafka/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Tutorial - Spring Boot \u0026 Apache Kafka","name":"Tutorial - Spring Boot \u0026 Apache Kafka","description":"A tutorial on how I created two Spring Boot applications that comunicate via Apache Kafka","keywords":["Java","Spring","Spring Boot","Apache Kafka","Event Streaming","Producer","Consumer"],"articleBody":"Yeah. If you’ve seen my last tutorial on AWS S3, you may be guessing that my wife is away for another weekend. But you guessed it wrong.\nThis time, my Technical Manager and I had the awesome idea to include Apache Kafka in our tech stack.\nAs the Senior dev and Tech Lead I took responsibility (and privilege) to work on this new integration.\nAs I did not yet know how to use Apache Kafka, I chose to begin my learning process by creating a quick POC (Proof of Concept) and to share what I’ve learned with you guys.\nWhat and Why Apache Kafka? In today’s world of distributed systems and microservices, traditional synchronous communication between services can often become a bottleneck. Enter Apache Kafka and other asynchronous messaging queues as scalable publish-subscribe models.\nApache Kafka, more than other tools, is built to persist, replicate and stream massive volumes of data with high fault tolerance, that’s why it’s so often chosen ad the messaging queue nowadays. And that’s why I’m so happy to learn it.\nThe theory behind it At its core, Kafka enables systems to communicate through events (also called messages) in a publish-subscribe model. Here’s a breakdown of key concepts:\nTopics A topic is a category or feed name to which records are sent by producers and from which consumers read. You can think of a topic as a log file or a stream of events grouped by a common purpose.\nKafka topics can be partitioned and replicated for scalability and fault tolerance.\nProducers (Publishers) A producer is any application or service that sends messages (also called publishing) to a Kafka topic. Producers can send data to different topics, and they decide which partition of a topic to write to — either randomly, round-robin, or based on a key.\nConsumers A consumer is any application or service that reads messages from one or more topics. Kafka delivers messages in the order they appear in each partition. Consumers can be grouped into consumer groups, where each group coordinates to read from a topic without duplicating work.\nStep 1 - Getting an Apache Kafka Instance Up and Running The Apache Kafka documentation webpage and even the Quickstart mention a couple of ways to download, install and run Kafka.\nMe? I always just look for the word “Docker” in any quickstart guide and use it. Here is what I wrote on a getAndRunKafka.sh shellscript to make it work:\n#!/bin/bash docker pull apache/kafka:4.0.0 docker run -d -p 9092:9092 apache/kafka:4.0.0 And yeah, that’s all I did to have an instance of Kafka running on my machine.\nStep 2 - Creating your Producer Application We usually think of communication as a Client and Server relationship. With Event Streaming, we talk about Producers and Consumers. The relationship is a little different overall.\nProducers, obviously, produce the messages and add them to the Topic. The Consumer will, again, obviously, consume the messages from that Topic.\nTherefore, it would make absolutely no sense to start developing on the Consumer side.\n2.1 - Create a Project with Spring Initializr I use the Spring Initializr via the Spring Boot Extension Pack on VSCode. You can always do it via the web, create your project, and unzip it to start coding. But what you’re going to do is:\nCreate a Maven project Spring Boot Version 3.4.3 Java Name it as you wish I’ve used Java 17 and JAR packaging You’ll add dependencies: Spring for Apache Kafka, Spring Web (optional), Spring Boot Dev Tools (optional), Lombok (optional) And you’re good to go.\n2.2 - Creating your Message I believe the most important part of this tutorial is the Kafka configuration and usage, but I do need the message POJO ready to configure it. So, this will be our POJO:\n@Data public class MessageModel { private String name; private LocalDateTime time; private String message; } 2.3 - Configuring for Apache Kafka So, now we’re adding some stuff to our configurations file, also know as application.properties:\nspring.application.name=producer server.port=8081 spring.kafka.bootstrap-servers=localhost:9092 The server port is here just to not conflict with the consumer (and with other applications in my machine hehe).\nThe most important thing is the address for the Kafka server.\nOur next configuration file is a Java file I will name KafkaProducerConfig.java:\n@Configuration public class KafkaProducerConfig { @Value(value = \"${spring.kafka.bootstrap-servers}\") private String serverAddress; @Bean public ProducerFactory messageProducerFactory() { Map configProps = new HashMap\u003c\u003e(); // ... return new DefaultKafkaProducerFactory\u003c\u003e(configProps); } @Bean public KafkaTemplate messageKafkaTemplate() { return new KafkaTemplate\u003c\u003e(messageProducerFactory()); } } This will be our baseline. Here we’re fetching the server address from the properties file. We are also going to generate a configuration map for our message production. Finally, we are generating a KafkaTemplate using the factory we configured. That KafkaTemplate is what we’ll use later to send the messages.\nOur configuration method is currently empty, this is what we’re adding to it:\n@Bean public ProducerFactory messageProducerFactory() { Map configProps = new HashMap\u003c\u003e(); configProps.put(JsonSerializer.ADD_TYPE_INFO_HEADERS, false); // this make communication more flexible configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, serverAddress); // server address configuration configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); // Key will be a String configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class); // Message will be an Object return new DefaultKafkaProducerFactory\u003c\u003e(configProps); } We still need one more file to configure our Topic, we’ll call it KafkaTopicConfig.java:\n@Configuration public class KafkaTopicConfig { @Getter private static final String TOPIC_NAME = \"douglas-messages\"; private static final int NUM_PARTITIONS = 1; private static final short REPLICATION_STRATEGY = 1; // no replication @Value(value = \"${spring.kafka.bootstrap-servers}\") private String serverAddress; @Bean public KafkaAdmin kafkaAdmin() { Map configs = new HashMap\u003c\u003e(); configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, serverAddress); return new KafkaAdmin(configs); } @Bean public NewTopic topicMessages() { return new NewTopic(TOPIC_NAME, NUM_PARTITIONS, REPLICATION_STRATEGY); } } Here we are also fetching the server address so we can create out Topic on the right Kafka instance. We are also creating the Topic with the name and configurations we want.\nIf you’re still wondering who will create that topic, you need to understand Spring Framework a little better.\n2.4 - Sending the Message We have already configured how our application will communicate via Kafka, but not when it will do it.\nFor that, we will create a MessageService.java that will send our messages via our topic.\n@Service public class MessageService { @Autowired private KafkaTemplate kafkaTemplate; public void sendMessage(MessageModel msg) { System.out.println(\"Sending message on Kafka Topic: \" + msg.toString()); msg.setTime(LocalDateTime.now()); kafkaTemplate.send(KafkaTopicConfig.getTOPIC_NAME(), msg); } } This should be more than enough for us to send messages via Apache Kafka with Spring Boot. The next section is optional but will make easier for us to trigger the sending action.\n2.5 - Triggering via POST (Optional) If you’re implementing this in a real project, this should do for your Producer. If you’re following this tutorial with me, we need some way to trigger the sending of those messages.\nI chose to do that with a Rest Controller. This is what our MessageController.java looks like:\n@RestController @RequestMapping(\"/message\") public class MessageController { @Autowired private MessageService service; @PostMapping(\"/\") public void postMessage(@RequestBody MessageModel msg) { service.sendMessage(msg); } } Now, if I use Postman to send a Post Request to our endpoint with this data:\n{ \"name\": \"Douglas\", \"time\": null, \"message\": \"Olá, mundo!\" } This gets printed on the Terminal:\nSending message on Kafka Topic: MessageModel(name=Douglas, time=2025-04-07T18:01:37.907616782, message=Olá, mundo!)\n2.6 - Inspecting via Console (Optional) This is another optional step that will only work if you have followed the last (or, if you are doing all of this in an existing project, it should work on your topics as well).\nAs we don’t yet have developed our Consumer, we need another way to verify if our Producer is… you know, producing.\nKafka, when built, makes some shellcript files available for us that will help us do that.\nIf you’ve built Kafka like I have, you can access them this way, via your terminal / console.\ndocker ps # this will let you find the container id where Kafka is running docker exec -t bash # fill in your container id # now you should be inside the container environment cd /opt/kafka/bin ./kafka-topics.sh --bootstrap-server localhost:9092 --list # this should list the topics /kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic --from-beginning It may take a couple of seconds, but this last command should let you see every message that was produced and sent on that topic.\nStep 3 - Creating your Consumer Application Now we have a functional Producer Application, we should consume the data from the Topic via Java as well.\n3.1 - Create a Project with Spring Initializr This should be done the exact same way as with the Producer. Reference 2.1 - Create a Project with Spring Initializr.\n3.2 - Creating your Message We also have to define the format of our message via POJO, so reference 2.2 - Creating your Message.\n3.3 - Configuring for Apache Kafka Here we are finally doing something different.\nFirst of all, you should add some configurations to your properties file. They are similar to the ones on the producer, but different.\nspring.application.name=consumer server.port=8082 spring.kafka.bootstrap-servers=localhost:9092 We used the Producer to configure the Topic, so we don not have to do that again. We just need to configure how we are going to read from the Topic we are producing on. We will do that on the KafkaConsumerConfig.java file.\n@EnableKafka @Configuration public class KafkaConsumerConfig { @Getter private static final String MESSAGE_TOPIC = \"douglas-messages\"; @Value(value = \"${spring.kafka.bootstrap-servers}\") private String serverAddress; @Bean public ConsumerFactory messageConsumerFactory() { Map configProps = new HashMap\u003c\u003e(); // ... return new DefaultKafkaConsumerFactory\u003c\u003e(configProps); } @Bean public ConcurrentKafkaListenerContainerFactory messageKafkaListenerContainerFactory() { ConcurrentKafkaListenerContainerFactory listenerFactory = new ConcurrentKafkaListenerContainerFactory\u003c\u003e(); listenerFactory.setConsumerFactory(messageConsumerFactory()); return listenerFactory; } } You can see the structure is very similar to the one on the Producer. We also have the server from the properties and the topic name on a variable to avoid magic Strings. We have a configuration method we are looking closer into next. And, finally, we have the ConcurrentKafkaListenerContainerFactoryCreatorOfEverythingQueenOfDragonsKhaleesi thing.\nYeah, huge name.\nBut what is does is mostly generating the Consumer version of the Kafka Template. The usage is a bit different, as we’ll see shortly. Now, to the configuration:\n@Bean public ConsumerFactory messageConsumerFactory() { Map configProps = new HashMap\u003c\u003e(); configProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, serverAddress); // setting the Server configProps.put(ConsumerConfig.GROUP_ID_CONFIG, MESSAGE_TOPIC); // setting the Topic // The next two configurations set an Error Handling layer on the consumer configProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class); configProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class); // We need to deserialize the result of the error handler when there's no error configProps.put(ErrorHandlingDeserializer.KEY_DESERIALIZER_CLASS, JsonDeserializer.class); configProps.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class); configProps.put(JsonDeserializer.TRUSTED_PACKAGES, \"*\"); // this removes a security layer we don't need configProps.put(JsonDeserializer.VALUE_DEFAULT_TYPE, MessageModel.class.getName()); // setting the model return new DefaultKafkaConsumerFactory\u003c\u003e(configProps); } There’s some more configuration here, but I believe the comments do justice.\n3.4 - Listening for Messages As we did for the Producer, the Consumer will have a MessageService.java file to listen for messages on that topic and act when there is a new one.\n@Service public class MessageService { @KafkaListener(topics = \"douglas-messages\", containerFactory = \"messageKafkaListenerContainerFactory\") public void messageListener(MessageModel msg) { System.out.println(\"Message Received by Consumer: \" + msg.toString()); } } You have probably noticed a big difference between the Producer and the Consumer now. But, overall, both codes are preetty clean and easy to understand.\nStep 4 - Making it Happen Now we are finally making the cool stuff happen.\nIf you’ve followed everything closely so far, you should be able to run both applications and make them work together.\nOn this tutorial, we are doing that with a Postman POST request sent to the Producer. This is the result:\n#This is what I sent via Postman { \"name\": \"Producer\", \"time\": null, \"message\": \"Hello, Consumer!\" } # This shows on the Producers Console Sending message on Kafka Topic: MessageModel(name=Producer, time=2025-04-08T14:12:08.338257923, message=Hello, Consumer!) # This shows on the Consumers Console Message Received by Consumer: MessageModel(name=Producer, time=2025-04-08T14:12:08.338257923, message=Hello, Consumer!) We’ve just built a basic but complete event-driven architecture using Apache Kafka and Spring Boot.\nThis setup, while simple, demonstrates the power of Kafka to decouple services and handle asynchronous communication with ease. In real-world applications, this approach can scale to thousands of messages per second, spanning microservices, data pipelines, logs, and more.\nIf you’re coming from a traditional REST-based mindset, as I am, this might feel like a paradigm shift — and that’s because it is. But it’s one that unlocks a whole new level of flexibility and robustness in your system design.\nReferences Spring Boot Documentation https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/ The official Spring Boot documentation for configuration, service layers, and REST controllers.\nApache Kafka Documentation https://kafka.apache.org/documentation/ The official Apache Kafka documentation with Java specific instructions.\nEXEMPLO PRÁTICO DO USO DO APACHE KAFKA COM JAVA SPRING BOOT https://www.youtube.com/watch?v=EKj8lDRgvLc\u0026ab_channel=Prof.Rog%C3%A9rioNapole%C3%A3oJr. This is a tutorial in Portuguese that helped me get this thing up and running for the first time.\n","wordCount":"2105","inLanguage":"en","image":"https://douglas-rocha-dev.vercel.app/images/cool-setup-002.jpeg","datePublished":"2025-04-17T00:00:00Z","dateModified":"2025-04-17T00:00:00Z","author":{"@type":"Person","name":"Douglas"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://douglas-rocha-dev.vercel.app/projects/spring-kafka/"},"publisher":{"@type":"Organization","name":"DouglasDev","logo":{"@type":"ImageObject","url":"https://douglas-rocha-dev.vercel.app/images/code-icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://douglas-rocha-dev.vercel.app/ accesskey=h title="Home (Alt + H)"><img src=https://douglas-rocha-dev.vercel.app/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=https://douglas-rocha-dev.vercel.app/about/ title=About><span>About</span></a></li><li><a href=https://douglas-rocha-dev.vercel.app/blog/ title=Blog><span>Blog</span></a></li><li><a href=https://douglas-rocha-dev.vercel.app/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://douglas-rocha-dev.vercel.app/contact/ title=Contact><span>Contact</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://douglas-rocha-dev.vercel.app/>Home</a>&nbsp;»&nbsp;<a href=https://douglas-rocha-dev.vercel.app/projects/>Projects</a></div><h1 class="post-title entry-hint-parent">Tutorial - Spring Boot & Apache Kafka</h1><div class=post-description>A tutorial on how I created two Spring Boot applications that comunicate via Apache Kafka</div><div class=post-meta><span title='2025-04-17 00:00:00 +0000 UTC'>April 17, 2025</span>&nbsp;·&nbsp;10 min&nbsp;·&nbsp;2105 words&nbsp;·&nbsp;Douglas</div></header><figure class=entry-cover><img loading=eager src=https://douglas-rocha-dev.vercel.app/images/cool-setup-002.jpeg alt="Cool Setup"><p>Unfortunately not my setup</p></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-and-why-apache-kafka>What and Why Apache Kafka?</a><ul><li><a href=#the-theory-behind-it>The theory behind it</a></li></ul></li><li><a href=#step-1---getting-an-apache-kafka-instance-up-and-running>Step 1 - Getting an Apache Kafka Instance Up and Running</a></li><li><a href=#step-2---creating-your-producer-application>Step 2 - Creating your Producer Application</a><ul><li><a href=#21---create-a-project-with-spring-initializr>2.1 - Create a Project with Spring Initializr</a></li><li><a href=#22---creating-your-message>2.2 - Creating your Message</a></li><li><a href=#23---configuring-for-apache-kafka>2.3 - Configuring for Apache Kafka</a></li><li><a href=#24---sending-the-message>2.4 - Sending the Message</a></li><li><a href=#25---triggering-via-post-optional>2.5 - Triggering via POST (Optional)</a></li><li><a href=#26---inspecting-via-console-optional>2.6 - Inspecting via Console (Optional)</a></li></ul></li><li><a href=#step-3---creating-your-consumer-application>Step 3 - Creating your Consumer Application</a><ul><li><a href=#31---create-a-project-with-spring-initializr>3.1 - Create a Project with Spring Initializr</a></li><li><a href=#32---creating-your-message>3.2 - Creating your Message</a></li><li><a href=#33---configuring-for-apache-kafka>3.3 - Configuring for Apache Kafka</a></li><li><a href=#34---listening-for-messages>3.4 - Listening for Messages</a></li></ul></li><li><a href=#step-4---making-it-happen>Step 4 - Making it Happen</a><ul><li><a href=#references>References</a></li></ul></li></ul></nav></div></details></div><div class=post-content><p>Yeah. If you&rsquo;ve seen my last tutorial on AWS S3, you may be guessing that my wife is away for another weekend. But you guessed it wrong.</p><p>This time, my Technical Manager and I had the awesome idea to include Apache Kafka in our tech stack.</p><p>As the Senior dev and Tech Lead I took responsibility (and privilege) to work on this new integration.</p><p>As I did not yet know how to use Apache Kafka, I chose to begin my learning process by creating a quick POC (Proof of Concept) and to share what I&rsquo;ve learned with you guys.</p><h2 id=what-and-why-apache-kafka>What and Why Apache Kafka?<a hidden class=anchor aria-hidden=true href=#what-and-why-apache-kafka>#</a></h2><p>In today&rsquo;s world of distributed systems and microservices, traditional synchronous communication between services can often become a bottleneck. Enter Apache Kafka and other asynchronous messaging queues as scalable <strong>publish-subscribe models</strong>.</p><p>Apache Kafka, more than other tools, is built to persist, replicate and stream massive volumes of data with high fault tolerance, that&rsquo;s why it&rsquo;s so often chosen ad the messaging queue nowadays. And that&rsquo;s why I&rsquo;m so happy to learn it.</p><h3 id=the-theory-behind-it>The theory behind it<a hidden class=anchor aria-hidden=true href=#the-theory-behind-it>#</a></h3><p>At its core, Kafka enables systems to communicate through <strong>events</strong> (also called <strong>messages</strong>) in a publish-subscribe model. Here&rsquo;s a breakdown of key concepts:</p><p><strong>Topics</strong>
A topic is a category or feed name to which records are sent by <strong>producers</strong> and from which <strong>consumers</strong> read. You can think of a topic as a log file or a stream of events grouped by a common purpose.</p><p>Kafka topics can be partitioned and replicated for scalability and fault tolerance.</p><p><strong>Producers (Publishers)</strong>
A producer is any application or service that <strong>sends messages (also called publishing)</strong> to a Kafka topic. Producers can send data to different topics, and they decide which partition of a topic to write to — either randomly, round-robin, or based on a key.</p><p><strong>Consumers</strong>
A consumer is any application or service that <strong>reads messages</strong> from one or more topics. Kafka delivers messages in the order they appear in each partition. Consumers can be grouped into consumer groups, where each group coordinates to read from a topic without duplicating work.</p><h2 id=step-1---getting-an-apache-kafka-instance-up-and-running>Step 1 - Getting an Apache Kafka Instance Up and Running<a hidden class=anchor aria-hidden=true href=#step-1---getting-an-apache-kafka-instance-up-and-running>#</a></h2><p>The <a href=https://kafka.apache.org/documentation/><strong>Apache Kafka documentation webpage</strong></a> and even the <a href=https://kafka.apache.org/quickstart><strong>Quickstart</strong></a> mention a couple of ways to download, install and run Kafka.</p><p>Me? I always just look for the word &ldquo;Docker&rdquo; in any quickstart guide and use it. Here is what I wrote on a <code>getAndRunKafka.sh</code> shellscript to make it work:</p><pre tabindex=0><code>#!/bin/bash

docker pull apache/kafka:4.0.0

docker run -d -p 9092:9092 apache/kafka:4.0.0
</code></pre><p>And yeah, that&rsquo;s all I did to have an instance of Kafka running on my machine.</p><h2 id=step-2---creating-your-producer-application>Step 2 - Creating your Producer Application<a hidden class=anchor aria-hidden=true href=#step-2---creating-your-producer-application>#</a></h2><p>We usually think of communication as a Client and Server relationship. With Event Streaming, we talk about Producers and Consumers. The relationship is a little different overall.</p><p>Producers, obviously, produce the messages and add them to the Topic. The Consumer will, again, obviously, consume the messages from that Topic.</p><p>Therefore, it would make absolutely no sense to start developing on the Consumer side.</p><h3 id=21---create-a-project-with-spring-initializr>2.1 - Create a Project with Spring Initializr<a hidden class=anchor aria-hidden=true href=#21---create-a-project-with-spring-initializr>#</a></h3><p>I use the Spring Initializr via the Spring Boot Extension Pack on VSCode. You can always do it via the web, create your project, and unzip it to start coding. But what you&rsquo;re going to do is:</p><ol><li>Create a Maven project</li><li>Spring Boot Version 3.4.3</li><li>Java</li><li>Name it as you wish</li><li>I&rsquo;ve used Java 17 and JAR packaging</li><li>You&rsquo;ll add dependencies: Spring for Apache Kafka, Spring Web (optional), Spring Boot Dev Tools (optional), Lombok (optional)</li></ol><p>And you&rsquo;re good to go.</p><h3 id=22---creating-your-message>2.2 - Creating your Message<a hidden class=anchor aria-hidden=true href=#22---creating-your-message>#</a></h3><p>I believe the most important part of this tutorial is the Kafka configuration and usage, but I do need the message POJO ready to configure it. So, this will be our POJO:</p><pre tabindex=0><code>@Data
public class MessageModel {
  private String name;
  private LocalDateTime time;
  private String message;
}
</code></pre><h3 id=23---configuring-for-apache-kafka>2.3 - Configuring for Apache Kafka<a hidden class=anchor aria-hidden=true href=#23---configuring-for-apache-kafka>#</a></h3><p>So, now we&rsquo;re adding some stuff to our configurations file, also know as <code>application.properties</code>:</p><pre tabindex=0><code>spring.application.name=producer
server.port=8081

spring.kafka.bootstrap-servers=localhost:9092
</code></pre><p>The server port is here just to not conflict with the consumer (and with other applications in my machine hehe).</p><p>The most important thing is the address for the Kafka server.</p><p>Our next configuration file is a Java file I will name <code>KafkaProducerConfig.java</code>:</p><pre tabindex=0><code>@Configuration
public class KafkaProducerConfig {

  @Value(value = &#34;${spring.kafka.bootstrap-servers}&#34;)
  private String serverAddress;

  @Bean
  public ProducerFactory&lt;String, MessageModel&gt; messageProducerFactory() {
    Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;();
    // ...
    return new DefaultKafkaProducerFactory&lt;&gt;(configProps);
  }

  @Bean
  public KafkaTemplate&lt;String, MessageModel&gt; messageKafkaTemplate() {
    return new KafkaTemplate&lt;&gt;(messageProducerFactory());
  }
}
</code></pre><p>This will be our baseline. Here we&rsquo;re fetching the server address from the properties file. We are also going to generate a configuration map for our message production. Finally, we are generating a KafkaTemplate using the factory we configured. That KafkaTemplate is what we&rsquo;ll use later to send the messages.</p><p>Our configuration method is currently empty, this is what we&rsquo;re adding to it:</p><pre tabindex=0><code>@Bean
public ProducerFactory&lt;String, MessageModel&gt; messageProducerFactory() {
    Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;();
    configProps.put(JsonSerializer.ADD_TYPE_INFO_HEADERS, false); // this make communication more flexible
    configProps.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, serverAddress); // server address configuration
    configProps.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class); // Key will be a String
    configProps.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, JsonSerializer.class); // Message will be an Object
    return new DefaultKafkaProducerFactory&lt;&gt;(configProps);
}
</code></pre><p>We still need one more file to configure our Topic, we&rsquo;ll call it <code>KafkaTopicConfig.java</code>:</p><pre tabindex=0><code>@Configuration
public class KafkaTopicConfig {

  @Getter
  private static final String TOPIC_NAME = &#34;douglas-messages&#34;;
  private static final int NUM_PARTITIONS = 1;
  private static final short REPLICATION_STRATEGY = 1; // no replication

  @Value(value = &#34;${spring.kafka.bootstrap-servers}&#34;)
  private String serverAddress;

  @Bean
  public KafkaAdmin kafkaAdmin() {
    Map&lt;String, Object&gt; configs = new HashMap&lt;&gt;();
    configs.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, serverAddress);
    return new KafkaAdmin(configs);
  }

  @Bean
  public NewTopic topicMessages() {
    return new NewTopic(TOPIC_NAME, NUM_PARTITIONS, REPLICATION_STRATEGY);
  }
}
</code></pre><p>Here we are also fetching the server address so we can create out Topic on the right Kafka instance. We are also creating the Topic with the name and configurations we want.</p><p>If you&rsquo;re still wondering who will create that topic, you need to understand Spring Framework a little better.</p><h3 id=24---sending-the-message>2.4 - Sending the Message<a hidden class=anchor aria-hidden=true href=#24---sending-the-message>#</a></h3><p>We have already configured how our application will communicate via Kafka, but not when it will do it.</p><p>For that, we will create a <code>MessageService.java</code> that will send our messages via our topic.</p><pre tabindex=0><code>@Service
public class MessageService {

  @Autowired
  private KafkaTemplate&lt;String, MessageModel&gt; kafkaTemplate;

  public void sendMessage(MessageModel msg) {
    System.out.println(&#34;Sending message on Kafka Topic: &#34; + msg.toString());
    msg.setTime(LocalDateTime.now());
    kafkaTemplate.send(KafkaTopicConfig.getTOPIC_NAME(), msg);
  }
}
</code></pre><p>This should be more than enough for us to send messages via Apache Kafka with Spring Boot. The next section is optional but will make easier for us to trigger the sending action.</p><h3 id=25---triggering-via-post-optional>2.5 - Triggering via POST (Optional)<a hidden class=anchor aria-hidden=true href=#25---triggering-via-post-optional>#</a></h3><p>If you&rsquo;re implementing this in a real project, this should do for your Producer. If you&rsquo;re following this tutorial with me, we need some way to trigger the sending of those messages.</p><p>I chose to do that with a Rest Controller. This is what our <code>MessageController.java</code> looks like:</p><pre tabindex=0><code>@RestController
@RequestMapping(&#34;/message&#34;)
public class MessageController {

  @Autowired
  private MessageService service;

  @PostMapping(&#34;/&#34;)
  public void postMessage(@RequestBody MessageModel msg) {
    service.sendMessage(msg);
  }
}
</code></pre><p>Now, if I use Postman to send a Post Request to our endpoint with this data:</p><pre tabindex=0><code>{
    &#34;name&#34;: &#34;Douglas&#34;,
    &#34;time&#34;: null,
    &#34;message&#34;: &#34;Olá, mundo!&#34;
}
</code></pre><p>This gets printed on the Terminal:</p><p><code>Sending message on Kafka Topic: MessageModel(name=Douglas, time=2025-04-07T18:01:37.907616782, message=Olá, mundo!)</code></p><h3 id=26---inspecting-via-console-optional>2.6 - Inspecting via Console (Optional)<a hidden class=anchor aria-hidden=true href=#26---inspecting-via-console-optional>#</a></h3><p>This is another optional step that will only work if you have followed the last (or, if you are doing all of this in an existing project, it should work on your topics as well).</p><p>As we don&rsquo;t yet have developed our Consumer, we need another way to verify if our Producer is&mldr; you know, producing.</p><p>Kafka, when built, makes some shellcript files available for us that will help us do that.</p><p>If you&rsquo;ve built Kafka like I have, you can access them this way, via your terminal / console.</p><pre tabindex=0><code>docker ps # this will let you find the container id where Kafka is running
docker exec -t &lt;CONTAINER-ID&gt; bash # fill in your container id
# now you should be inside the container environment
cd /opt/kafka/bin
./kafka-topics.sh --bootstrap-server localhost:9092 --list # this should list the topics
/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic &lt;TOPIC-NAME&gt; --from-beginning
</code></pre><p>It may take a couple of seconds, but this last command should let you see every message that was produced and sent on that topic.</p><h2 id=step-3---creating-your-consumer-application>Step 3 - Creating your Consumer Application<a hidden class=anchor aria-hidden=true href=#step-3---creating-your-consumer-application>#</a></h2><p>Now we have a functional Producer Application, we should consume the data from the Topic via Java as well.</p><h3 id=31---create-a-project-with-spring-initializr>3.1 - Create a Project with Spring Initializr<a hidden class=anchor aria-hidden=true href=#31---create-a-project-with-spring-initializr>#</a></h3><p>This should be done the exact same way as with the Producer. Reference <a href=#21---create-a-project-with-spring-initializr>2.1 - Create a Project with Spring Initializr</a>.</p><h3 id=32---creating-your-message>3.2 - Creating your Message<a hidden class=anchor aria-hidden=true href=#32---creating-your-message>#</a></h3><p>We also have to define the format of our message via POJO, so reference <a href=#22---creating-your-message>2.2 - Creating your Message</a>.</p><h3 id=33---configuring-for-apache-kafka>3.3 - Configuring for Apache Kafka<a hidden class=anchor aria-hidden=true href=#33---configuring-for-apache-kafka>#</a></h3><p>Here we are finally doing something different.</p><p>First of all, you should add some configurations to your properties file. They are similar to the ones on the producer, but different.</p><pre tabindex=0><code>spring.application.name=consumer
server.port=8082

spring.kafka.bootstrap-servers=localhost:9092
</code></pre><p>We used the Producer to configure the Topic, so we don not have to do that again. We just need to configure how we are going to read from the Topic we are producing on. We will do that on the <code>KafkaConsumerConfig.java</code> file.</p><pre tabindex=0><code>@EnableKafka
@Configuration
public class KafkaConsumerConfig {

  @Getter
  private static final String MESSAGE_TOPIC = &#34;douglas-messages&#34;;

  @Value(value = &#34;${spring.kafka.bootstrap-servers}&#34;)
  private String serverAddress;

  @Bean
  public ConsumerFactory&lt;String, MessageModel&gt; messageConsumerFactory() {
    Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;();
    // ...
    return new DefaultKafkaConsumerFactory&lt;&gt;(configProps);
  }

  @Bean
  public ConcurrentKafkaListenerContainerFactory&lt;String, MessageModel&gt; messageKafkaListenerContainerFactory() {
    ConcurrentKafkaListenerContainerFactory&lt;String, MessageModel&gt; listenerFactory = new ConcurrentKafkaListenerContainerFactory&lt;&gt;();
    listenerFactory.setConsumerFactory(messageConsumerFactory());
    return listenerFactory;
  }
}
</code></pre><p>You can see the structure is very similar to the one on the Producer. We also have the server from the properties and the topic name on a variable to avoid magic Strings. We have a configuration method we are looking closer into next. And, finally, we have the <code>ConcurrentKafkaListenerContainerFactoryCreatorOfEverythingQueenOfDragonsKhaleesi</code> thing.</p><p>Yeah, huge name.</p><p>But what is does is mostly generating the Consumer version of the Kafka Template. The usage is a bit different, as we&rsquo;ll see shortly. Now, to the configuration:</p><pre tabindex=0><code>@Bean
  public ConsumerFactory&lt;String, MessageModel&gt; messageConsumerFactory() {
    Map&lt;String, Object&gt; configProps = new HashMap&lt;&gt;();

    configProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, serverAddress); // setting the Server
    configProps.put(ConsumerConfig.GROUP_ID_CONFIG, MESSAGE_TOPIC); // setting the Topic

    // The next two configurations set an Error Handling layer on the consumer
    configProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);
    configProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, ErrorHandlingDeserializer.class);

    // We need to deserialize the result of the error handler when there&#39;s no error
    configProps.put(ErrorHandlingDeserializer.KEY_DESERIALIZER_CLASS, JsonDeserializer.class);
    configProps.put(ErrorHandlingDeserializer.VALUE_DESERIALIZER_CLASS, JsonDeserializer.class);

    configProps.put(JsonDeserializer.TRUSTED_PACKAGES, &#34;*&#34;); // this removes a security layer we don&#39;t need
    configProps.put(JsonDeserializer.VALUE_DEFAULT_TYPE, MessageModel.class.getName()); // setting the model

    return new DefaultKafkaConsumerFactory&lt;&gt;(configProps);
  }
</code></pre><p>There&rsquo;s some more configuration here, but I believe the comments do justice.</p><h3 id=34---listening-for-messages>3.4 - Listening for Messages<a hidden class=anchor aria-hidden=true href=#34---listening-for-messages>#</a></h3><p>As we did for the Producer, the Consumer will have a <code>MessageService.java</code> file to listen for messages on that topic and act when there is a new one.</p><pre tabindex=0><code>@Service
public class MessageService {

  @KafkaListener(topics = &#34;douglas-messages&#34;, containerFactory = &#34;messageKafkaListenerContainerFactory&#34;)
  public void messageListener(MessageModel msg) {
    System.out.println(&#34;Message Received by Consumer: &#34; + msg.toString());
  }
}
</code></pre><p>You have probably noticed a big difference between the Producer and the Consumer now. But, overall, both codes are preetty clean and easy to understand.</p><h2 id=step-4---making-it-happen>Step 4 - Making it Happen<a hidden class=anchor aria-hidden=true href=#step-4---making-it-happen>#</a></h2><p>Now we are finally making the cool stuff happen.</p><p>If you&rsquo;ve followed everything closely so far, you should be able to run both applications and make them work together.</p><p>On this tutorial, we are doing that with a Postman POST request sent to the Producer. This is the result:</p><pre tabindex=0><code>#This is what I sent via Postman
{
    &#34;name&#34;: &#34;Producer&#34;,
    &#34;time&#34;: null,
    &#34;message&#34;: &#34;Hello, Consumer!&#34;
}

# This shows on the Producers Console
Sending message on Kafka Topic: MessageModel(name=Producer, time=2025-04-08T14:12:08.338257923, message=Hello, Consumer!)

# This shows on the Consumers Console
Message Received by Consumer: MessageModel(name=Producer, time=2025-04-08T14:12:08.338257923, message=Hello, Consumer!)
</code></pre><hr><p>We&rsquo;ve just built a basic but complete event-driven architecture using <strong>Apache Kafka and Spring Boot</strong>.</p><p>This setup, while simple, demonstrates the power of Kafka to decouple services and handle asynchronous communication with ease. In real-world applications, this approach can scale to thousands of messages per second, spanning microservices, data pipelines, logs, and more.</p><p>If you’re coming from a traditional REST-based mindset, as I am, this might feel like a paradigm shift — and that’s because it is. But it&rsquo;s one that unlocks a whole new level of flexibility and robustness in your system design.</p><h3 id=references>References<a hidden class=anchor aria-hidden=true href=#references>#</a></h3><ol><li><p>Spring Boot Documentation
<a href=https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/>https://docs.spring.io/spring-boot/docs/current/reference/htmlsingle/</a>
The official Spring Boot documentation for configuration, service layers, and REST controllers.</p></li><li><p>Apache Kafka Documentation
<a href=https://kafka.apache.org/documentation/>https://kafka.apache.org/documentation/</a>
The official Apache Kafka documentation with Java specific instructions.</p></li><li><p>EXEMPLO PRÁTICO DO USO DO APACHE KAFKA COM JAVA SPRING BOOT
<a href="https://www.youtube.com/watch?v=EKj8lDRgvLc&amp;ab_channel=Prof.Rog%C3%A9rioNapole%C3%A3oJr">https://www.youtube.com/watch?v=EKj8lDRgvLc&amp;ab_channel=Prof.Rog%C3%A9rioNapole%C3%A3oJr</a>.
This is a tutorial in Portuguese that helped me get this thing up and running for the first time.</p></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=https://douglas-rocha-dev.vercel.app/tags/java/>Java</a></li><li><a href=https://douglas-rocha-dev.vercel.app/tags/spring/>Spring</a></li><li><a href=https://douglas-rocha-dev.vercel.app/tags/spring-boot/>Spring Boot</a></li><li><a href=https://douglas-rocha-dev.vercel.app/tags/apache-kafka/>Apache Kafka</a></li><li><a href=https://douglas-rocha-dev.vercel.app/tags/event-streaming/>Event Streaming</a></li><li><a href=https://douglas-rocha-dev.vercel.app/tags/producer/>Producer</a></li><li><a href=https://douglas-rocha-dev.vercel.app/tags/consumer/>Consumer</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://douglas-rocha-dev.vercel.app/>DouglasDev</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>